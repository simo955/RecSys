{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2018/19\n",
    "\n",
    "### Practice 4 - Similarity with Cython\n",
    "\n",
    "\n",
    "### Cython is a superset of Python, allowing you to use C-like operations and import C code. Cython files (.pyx) are compiled and support static typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement something simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPrime(n):\n",
    "    \n",
    "    i = 2\n",
    "    \n",
    "    # Usually you loop up to sqrt(n)\n",
    "    while i < n:\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is prime 2? True\n",
      "Is prime 3? True\n",
      "Is prime 5? True\n",
      "Is prime 15? False\n",
      "Is prime 20? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Is prime 2? {}\".format(isPrime(2)))\n",
    "print(\"Is prime 3? {}\".format(isPrime(3)))\n",
    "print(\"Is prime 5? {}\".format(isPrime(5)))\n",
    "print(\"Is prime 15? {}\".format(isPrime(15)))\n",
    "print(\"Is prime 20? {}\".format(isPrime(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Prime 80000023? True, time required 8.19 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "result = isPrime(80000023)\n",
    "\n",
    "print(\"Is Prime 80000023? {}, time required {:.2f} sec\".format(result, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Cython magic command, this takes care of the compilation step. If you are writing code outside Jupyter you'll have to compile using other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare Cython function, paste the same code as before. The function will be compiled and then executed with a Python interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def isPrime(n):\n",
    "    \n",
    "    i = 2\n",
    "    \n",
    "    # Usually you loop up to sqrt(n)\n",
    "    while i < n:\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Prime 80000023? True, time required 4.81 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "result = isPrime(80000023)\n",
    "\n",
    "print(\"Is Prime 80000023? {}, time required {:.2f} sec\".format(result, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see by just compiling the same code we got some improvement.\n",
    "#### To go seriously higher, we have to use some static tiping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# Declare the tipe of the arguments\n",
    "def isPrime(long n):\n",
    "    \n",
    "    # Declare index of for loop\n",
    "    cdef long i\n",
    "    \n",
    "    i = 2\n",
    "    \n",
    "    # Usually you loop up to sqrt(n)\n",
    "    while i < n:\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Prime 80000023? True, time required 0.94 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "result = isPrime(80000023)\n",
    "\n",
    "print(\"Is Prime 80000023? {}, time required {:.2f} sec\".format(result, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cython code with two tipe declaration, for n and i, runs 50x faster than Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main benefits of Cython:\n",
    "* Compiled, no interpreter\n",
    "* Static typing, no overhead\n",
    "* Fast loops, no need to vectorize. Vectorization sometimes performes lots of useless operations\n",
    "* Numpy, which is fast in python, becomes often slooooow compared to a carefully written Cython code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity with Cython\n",
    "\n",
    "#### Load the usual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71568x65134 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10000054 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "# skip the download\n",
    "#urlretrieve (\"http://files.grouplens.org/datasets/movielens/ml-10m.zip\", \"data/Movielens_10M/movielens_10m.zip\")\n",
    "dataFile = zipfile.ZipFile(\"data/Movielens_10M/movielens_10m.zip\")\n",
    "URM_path = dataFile.extract(\"ml-10M100K/ratings.dat\", path = \"data/Movielens_10M\")\n",
    "URM_file = open(URM_path, 'r')\n",
    "\n",
    "\n",
    "def rowSplit (rowString):\n",
    "    \n",
    "    split = rowString.split(\"::\")\n",
    "    split[3] = split[3].replace(\"\\n\",\"\")\n",
    "    \n",
    "    split[0] = int(split[0])\n",
    "    split[1] = int(split[1])\n",
    "    split[2] = float(split[2])\n",
    "    split[3] = int(split[3])\n",
    "    \n",
    "    result = tuple(split)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "URM_file.seek(0)\n",
    "URM_tuples = []\n",
    "\n",
    "for line in URM_file:\n",
    "   URM_tuples.append(rowSplit (line))\n",
    "\n",
    "userList, itemList, ratingList, timestampList = zip(*URM_tuples)\n",
    "\n",
    "userList = list(userList)\n",
    "itemList = list(itemList)\n",
    "ratingList = list(ratingList)\n",
    "timestampList = list(timestampList)\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all = URM_all.tocsr()\n",
    "\n",
    "URM_all\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Notebooks_utils.data_splitter import train_test_holdout\n",
    "\n",
    "\n",
    "URM_train, URM_test = train_test_holdout(URM_all, train_perc = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71568x65134 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4997710 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we cannot store in memory the whole similarity, we compute it one row at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71568,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemIndex=1\n",
    "item_ratings = URM_train[:,itemIndex]\n",
    "item_ratings = item_ratings.toarray().squeeze()\n",
    "\n",
    "item_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65134,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_item_weights = URM_train.T.dot(item_ratings)\n",
    "this_item_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we have the scores for that row, we get the TopK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 589,  480,  110, 1210,  593,  318,  296,  356,  260,    1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=10\n",
    "\n",
    "top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "top_k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function hiding some conversion checks\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Basic Collaborative filtering recommender using only cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicItemKNN_CF_Recommender(object):\n",
    "    \"\"\" ItemKNN recommender with cosine similarity and no shrinkage\"\"\"\n",
    "\n",
    "    def __init__(self, URM):\n",
    "        self.dataset = URM\n",
    "        \n",
    "        \n",
    "    def compute_similarity(self, URM):\n",
    "        \n",
    "        # We explore the matrix column-wise\n",
    "        URM = check_matrix(URM, 'csc')     \n",
    "                    \n",
    "        values = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        processedItems = 0\n",
    "        \n",
    "        # Compute all similarities for each item using vectorization\n",
    "        for itemIndex in range(URM.shape[0]):\n",
    "            \n",
    "            processedItems += 1\n",
    "            \n",
    "            if processedItems % 100==0:\n",
    "                \n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "                \n",
    "                print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                    processedItems, itemPerSec, URM.shape[0]/itemPerSec/60))\n",
    "            \n",
    "            # All ratings for a given item\n",
    "            item_ratings = URM[:,itemIndex]\n",
    "            item_ratings = item_ratings.toarray().squeeze()\n",
    "            \n",
    "            # Compute item similarities\n",
    "            this_item_weights = URM_train.T.dot(item_ratings)\n",
    "            \n",
    "            # Sort indices and select TopK\n",
    "            top_k_idx = np.argsort(this_item_weights) [-self.k:]\n",
    "            \n",
    "            # Incrementally build sparse matrix\n",
    "            values.extend(this_item_weights[top_k_idx])\n",
    "            rows.extend(np.arange(URM.shape[0])[top_k_idx])\n",
    "            cols.extend(np.ones(self.k) * itemIndex)\n",
    "            \n",
    "        self.W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                                       shape=(URM.shape[0], URM.shape[0]),\n",
    "                                       dtype=np.float32)\n",
    "\n",
    "        \n",
    "\n",
    "    def fit(self, k=50, shrinkage=100):\n",
    "\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        \n",
    "        item_weights = self.compute_similarity(self.dataset)\n",
    "        \n",
    "        item_weights = check_matrix(item_weights, 'csr')\n",
    "        \n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's isolate the compute_similarity function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(URM, k=100):\n",
    "\n",
    "    # We explore the matrix column-wise\n",
    "    URM = check_matrix(URM, 'csc')\n",
    "    \n",
    "    n_items = URM.shape[0]\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    processedItems = 0\n",
    "\n",
    "    # Compute all similarities for each item using vectorization\n",
    "    # for itemIndex in range(n_items):\n",
    "    for itemIndex in range(1000):\n",
    "\n",
    "        processedItems += 1\n",
    "\n",
    "        if processedItems % 100==0:\n",
    "\n",
    "            itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "            print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                processedItems, itemPerSec, n_items/itemPerSec/60))\n",
    "\n",
    "        # All ratings for a given item\n",
    "        item_ratings = URM[:,itemIndex]\n",
    "        item_ratings = item_ratings.toarray().squeeze()\n",
    "\n",
    "        # Compute item similarities\n",
    "        this_item_weights = URM.T.dot(item_ratings)\n",
    "\n",
    "        # Sort indices and select TopK\n",
    "        top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "\n",
    "        # Incrementally build sparse matrix\n",
    "        values.extend(this_item_weights[top_k_idx])\n",
    "        rows.extend(np.arange(URM.shape[0])[top_k_idx])\n",
    "        cols.extend(np.ones(k) * itemIndex)\n",
    "\n",
    "    W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                            shape=(n_items, n_items),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    return W_sparse\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 100, 81.61 item/sec, required time 14.62 min\n",
      "Similarity item 200, 80.34 item/sec, required time 14.85 min\n",
      "Similarity item 300, 80.08 item/sec, required time 14.89 min\n",
      "Similarity item 400, 80.50 item/sec, required time 14.82 min\n",
      "Similarity item 500, 80.02 item/sec, required time 14.91 min\n",
      "Similarity item 600, 80.30 item/sec, required time 14.85 min\n",
      "Similarity item 700, 80.23 item/sec, required time 14.87 min\n",
      "Similarity item 800, 80.58 item/sec, required time 14.80 min\n",
      "Similarity item 900, 81.18 item/sec, required time 14.69 min\n",
      "Similarity item 1000, 81.15 item/sec, required time 14.70 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<71568x71568 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 100000 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_similarity(URM_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### We see that computing the similarity takes more or less 15 minutes\n",
    "### Now we use the same identical code, but we compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "def compute_similarity_compiled(URM, k=100):\n",
    "\n",
    "    # We explore the matrix column-wise\n",
    "    URM = URM.tocsc()\n",
    "    \n",
    "    n_items = URM.shape[0]\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    processedItems = 0\n",
    "\n",
    "    # Compute all similarities for each item using vectorization\n",
    "    # for itemIndex in range(n_items):\n",
    "    for itemIndex in range(1000):\n",
    "\n",
    "        processedItems += 1\n",
    "\n",
    "        if processedItems % 100==0:\n",
    "\n",
    "            itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "            print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                processedItems, itemPerSec, n_items/itemPerSec/60))\n",
    "\n",
    "        # All ratings for a given item\n",
    "        item_ratings = URM[:,itemIndex]\n",
    "        item_ratings = item_ratings.toarray().squeeze()\n",
    "\n",
    "        # Compute item similarities\n",
    "        this_item_weights = URM.T.dot(item_ratings)\n",
    "\n",
    "        # Sort indices and select TopK\n",
    "        top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "\n",
    "        # Incrementally build sparse matrix\n",
    "        values.extend(this_item_weights[top_k_idx])\n",
    "        rows.extend(np.arange(URM.shape[0])[top_k_idx])\n",
    "        cols.extend(np.ones(k) * itemIndex)\n",
    "\n",
    "    W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                            shape=(n_items, n_items),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    return W_sparse\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 100, 56.48 item/sec, required time 21.12 min\n",
      "Similarity item 200, 56.12 item/sec, required time 21.25 min\n",
      "Similarity item 300, 56.58 item/sec, required time 21.08 min\n",
      "Similarity item 400, 56.42 item/sec, required time 21.14 min\n",
      "Similarity item 500, 56.74 item/sec, required time 21.02 min\n",
      "Similarity item 600, 56.90 item/sec, required time 20.96 min\n",
      "Similarity item 700, 56.90 item/sec, required time 20.96 min\n",
      "Similarity item 800, 56.97 item/sec, required time 20.94 min\n",
      "Similarity item 900, 56.84 item/sec, required time 20.99 min\n",
      "Similarity item 1000, 56.57 item/sec, required time 21.08 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<71568x71568 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 100000 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_similarity_compiled(URM_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### As opposed to the previous example, compilation by itself is not very helpful. Why?\n",
    "#### Because the compiler is just porting in C all operations that the python interpreter would have to perform, dynamic tiping included\n",
    "\n",
    "### Now try to add some tipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "cimport numpy as np\n",
    "\n",
    "def compute_similarity_compiled(URM, int k=100):\n",
    "    \n",
    "    cdef int itemIndex, processedItems\n",
    "    \n",
    "    # We use the numpy syntax, allowing us to perform vectorized operations\n",
    "    cdef np.ndarray[double, ndim=1] item_ratings, this_item_weights\n",
    "    cdef np.ndarray[long, ndim=1] top_k_idx\n",
    "\n",
    "    # We explore the matrix column-wise\n",
    "    URM = URM.tocsc()\n",
    "    \n",
    "    n_items = URM.shape[0]\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    processedItems = 0\n",
    "\n",
    "    # Compute all similarities for each item using vectorization\n",
    "    # for itemIndex in range(n_items):\n",
    "    for itemIndex in range(1000):\n",
    "\n",
    "        processedItems += 1\n",
    "\n",
    "        if processedItems % 100==0:\n",
    "\n",
    "            itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "            print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                processedItems, itemPerSec, n_items/itemPerSec/60))\n",
    "\n",
    "        # All ratings for a given item\n",
    "        item_ratings = URM[:,itemIndex].toarray().squeeze()\n",
    "\n",
    "        # Compute item similarities\n",
    "        this_item_weights = URM.T.dot(item_ratings)\n",
    "\n",
    "        # Sort indices and select TopK\n",
    "        top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "\n",
    "        # Incrementally build sparse matrix\n",
    "        values.extend(this_item_weights[top_k_idx])\n",
    "        rows.extend(np.arange(URM.shape[0])[top_k_idx])\n",
    "        cols.extend(np.ones(k) * itemIndex)\n",
    "\n",
    "    W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                            shape=(n_items, n_items),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    return W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 100, 57.80 item/sec, required time 20.64 min\n",
      "Similarity item 200, 53.69 item/sec, required time 22.22 min\n",
      "Similarity item 300, 54.57 item/sec, required time 21.86 min\n",
      "Similarity item 400, 54.07 item/sec, required time 22.06 min\n",
      "Similarity item 500, 54.65 item/sec, required time 21.83 min\n",
      "Similarity item 600, 54.82 item/sec, required time 21.76 min\n",
      "Similarity item 700, 55.08 item/sec, required time 21.66 min\n",
      "Similarity item 800, 55.30 item/sec, required time 21.57 min\n",
      "Similarity item 900, 55.64 item/sec, required time 21.44 min\n",
      "Similarity item 1000, 55.80 item/sec, required time 21.38 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<71568x71568 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 100000 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_similarity_compiled(URM_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still no luck! Why?\n",
    "### There are a few reasons:\n",
    "* We are getting the data from the sparse matrix using its interface, which is SLOW\n",
    "* We are transforming sparse data into a dense array, which is SLOW\n",
    "* We are performing a dot product against a dense vector\n",
    "\n",
    "#### You colud find a workaround... here we do something different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed solution\n",
    "### Change the algorithm!\n",
    "\n",
    "### Instead of performing the dot product, let's implement somenting that computes the similarity using sparse data directly\n",
    "\n",
    "### We loop through the data and update selectively the similarity matrix cells. \n",
    "### Underlying idea:\n",
    "* When I select an item I can know which users rated it\n",
    "* Instead of looping through the other items trying to find common users, I use the URM to find which other items that user rated\n",
    "* The user I am considering will be common between the two, so I increment the similarity of the two items\n",
    "* Instead of following the path item1 -> loop item2 -> find user, i go item1 -> loop user -> loop item2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 0, 1],\n",
       "        [0, 1, 1, 1],\n",
       "        [1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix = np.array([[1,1,0,1],[0,1,1,1],[1,0,1,0]])\n",
    "data_matrix = sps.csc_matrix(data_matrix)\n",
    "data_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Compute the similarities for item 1\n",
    "\n",
    "#### Step 1: get users that rated item 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_rated_item = data_matrix[:,1]\n",
    "users_rated_item.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: count how many times those users rated other items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_similarity = data_matrix[users_rated_item.indices].sum(axis = 0)\n",
    "np.array(item_similarity).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify our result against the common method. We can see that the similarity values for col 1 are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix_product = data_matrix.T.dot(data_matrix)\n",
    "similarity_matrix_product.toarray()[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code works for implicit feedback only\n",
    "def compute_similarity_new_algorithm(URM, k=100):\n",
    "\n",
    "    # We explore the matrix column-wise\n",
    "    URM = check_matrix(URM, 'csc')\n",
    "    URM.data = np.ones_like(URM.data)\n",
    "    \n",
    "    n_items = URM.shape[0]\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    processedItems = 0\n",
    "\n",
    "    # Compute all similarities for each item using vectorization\n",
    "    # for itemIndex in range(n_items):\n",
    "    for itemIndex in range(1000):\n",
    "\n",
    "        processedItems += 1\n",
    "\n",
    "        if processedItems % 100==0:\n",
    "\n",
    "            itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "            print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                processedItems, itemPerSec, n_items/itemPerSec/60))\n",
    "\n",
    "        # All ratings for a given item\n",
    "        users_rated_item = URM.indices[URM.indptr[itemIndex]:URM.indptr[itemIndex+1]]\n",
    "\n",
    "        # Compute item similarities\n",
    "        this_item_weights = URM[users_rated_item].sum(axis = 0)\n",
    "        this_item_weights = np.array(this_item_weights).squeeze()\n",
    "\n",
    "        # Sort indices and select TopK\n",
    "        top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "\n",
    "        # Incrementally build sparse matrix\n",
    "        values.extend(this_item_weights[top_k_idx])\n",
    "        rows.extend(np.arange(URM.shape[0])[top_k_idx])\n",
    "        cols.extend(np.ones(k) * itemIndex)\n",
    "\n",
    "    W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                            shape=(n_items, n_items),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    return W_sparse\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 100, 28.04 item/sec, required time 42.53 min\n",
      "Similarity item 200, 28.37 item/sec, required time 42.04 min\n",
      "Similarity item 300, 28.85 item/sec, required time 41.35 min\n",
      "Similarity item 400, 28.77 item/sec, required time 41.45 min\n",
      "Similarity item 500, 29.20 item/sec, required time 40.85 min\n",
      "Similarity item 600, 28.85 item/sec, required time 41.34 min\n",
      "Similarity item 700, 29.60 item/sec, required time 40.30 min\n",
      "Similarity item 800, 29.91 item/sec, required time 39.88 min\n",
      "Similarity item 900, 30.54 item/sec, required time 39.06 min\n",
      "Similarity item 1000, 30.61 item/sec, required time 38.96 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<71568x71568 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 100000 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_similarity_new_algorithm(URM_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slower but expected, dot product operations are implemented in an efficient way and here we are using an indirect approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's write this algorithm in Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "cdef class Cosine_Similarity:\n",
    "\n",
    "    cdef int TopK\n",
    "    cdef long n_items\n",
    "\n",
    "    # Arrays containing the sparse data\n",
    "    cdef int[:] user_to_item_row_ptr, user_to_item_cols\n",
    "    cdef int[:] item_to_user_rows, item_to_user_col_ptr\n",
    "    cdef double[:] user_to_item_data, item_to_user_data\n",
    "\n",
    "    # In case you select no TopK\n",
    "    cdef double[:,:] W_dense\n",
    "\n",
    "    \n",
    "    def __init__(self, URM, TopK = 100):\n",
    "        \"\"\"\n",
    "        Dataset must be a matrix with items as columns\n",
    "        :param dataset:\n",
    "        :param TopK:\n",
    "        \"\"\"\n",
    "\n",
    "        super(Cosine_Similarity, self).__init__()\n",
    "\n",
    "        self.n_items = URM.shape[1]\n",
    "\n",
    "        self.TopK = min(TopK, self.n_items)\n",
    "\n",
    "        URM = URM.tocsr()\n",
    "        self.user_to_item_row_ptr = URM.indptr\n",
    "        self.user_to_item_cols = URM.indices\n",
    "        self.user_to_item_data = np.array(URM.data, dtype=np.float64)\n",
    "\n",
    "        URM = URM.tocsc()\n",
    "        self.item_to_user_rows = URM.indices\n",
    "        self.item_to_user_col_ptr = URM.indptr\n",
    "        self.item_to_user_data = np.array(URM.data, dtype=np.float64)\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_items,self.n_items))\n",
    "\n",
    "\n",
    "\n",
    "    cdef int[:] getUsersThatRatedItem(self, long item_id):\n",
    "        return self.item_to_user_rows[self.item_to_user_col_ptr[item_id]:self.item_to_user_col_ptr[item_id+1]]\n",
    "\n",
    "    cdef int[:] getItemsRatedByUser(self, long user_id):\n",
    "        return self.user_to_item_cols[self.user_to_item_row_ptr[user_id]:self.user_to_item_row_ptr[user_id+1]]\n",
    "\n",
    "    \n",
    "    \n",
    "    cdef double[:] computeItemSimilarities(self, long item_id_input):\n",
    "        \"\"\"\n",
    "        For every item the cosine similarity against other items depends on whether they have users in common. \n",
    "        The more common users the higher the similarity.\n",
    "        \n",
    "        The basic implementation is:\n",
    "        - Select the first item\n",
    "        - Loop through all other items\n",
    "        -- Given the two items, get the users they have in common\n",
    "        -- Update the similarity considering all common users\n",
    "        \n",
    "        That is VERY slow due to the common user part, in which a long data structure is looped multiple times.\n",
    "        \n",
    "        A better way is to use the data structure in a different way skipping the search part, getting directly\n",
    "        the information we need.\n",
    "        \n",
    "        The implementation here used is:\n",
    "        - Select the first item\n",
    "        - Initialize a zero valued array for the similarities\n",
    "        - Get the users who rated the first item\n",
    "        - Loop through the users\n",
    "        -- Given a user, get the items he rated (second item)\n",
    "        -- Update the similarity of the items he rated\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Create template used to initialize an array with zeros\n",
    "        # Much faster than np.zeros(self.n_items)\n",
    "        cdef array[double] template_zero = array('d')\n",
    "        cdef array[double] result = clone(template_zero, self.n_items, zero=True)\n",
    "\n",
    "\n",
    "        cdef long user_index, user_id, item_index, item_id_second\n",
    "\n",
    "        cdef int[:] users_that_rated_item = self.getUsersThatRatedItem(item_id_input)\n",
    "        cdef int[:] items_rated_by_user\n",
    "\n",
    "        cdef double rating_item_input, rating_item_second\n",
    "\n",
    "        # Get users that rated the items\n",
    "        for user_index in range(len(users_that_rated_item)):\n",
    "\n",
    "            user_id = users_that_rated_item[user_index]\n",
    "            rating_item_input = self.item_to_user_data[self.item_to_user_col_ptr[item_id_input]+user_index]\n",
    "\n",
    "            # Get all items rated by that user\n",
    "            items_rated_by_user = self.getItemsRatedByUser(user_id)\n",
    "\n",
    "            for item_index in range(len(items_rated_by_user)):\n",
    "\n",
    "                item_id_second = items_rated_by_user[item_index]\n",
    "\n",
    "                # Do not compute the similarity on the diagonal\n",
    "                if item_id_second != item_id_input:\n",
    "                    # Increment similairty\n",
    "                    rating_item_second = self.user_to_item_data[self.user_to_item_row_ptr[user_id]+item_index]\n",
    "\n",
    "                    result[item_id_second] += rating_item_input*rating_item_second\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def compute_similarity(self):\n",
    "\n",
    "        cdef int itemIndex, innerItemIndex\n",
    "        cdef long long topKItemIndex\n",
    "\n",
    "        cdef long long[:] top_k_idx\n",
    "\n",
    "        # Declare numpy data type to use vetor indexing and simplify the topK selection code\n",
    "        cdef np.ndarray[long, ndim=1] top_k_partition, top_k_partition_sorting\n",
    "        cdef np.ndarray[np.float64_t, ndim=1] this_item_weights_np\n",
    "\n",
    "        #cdef long[:] top_k_idx\n",
    "        cdef double[:] this_item_weights\n",
    "\n",
    "        cdef long processedItems = 0\n",
    "\n",
    "        # Data structure to incrementally build sparse matrix\n",
    "        # Preinitialize max possible length\n",
    "        cdef double[:] values = np.zeros((self.n_items*self.TopK))\n",
    "        cdef int[:] rows = np.zeros((self.n_items*self.TopK,), dtype=np.int32)\n",
    "        cdef int[:] cols = np.zeros((self.n_items*self.TopK,), dtype=np.int32)\n",
    "        cdef long sparse_data_pointer = 0\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Compute all similarities for each item\n",
    "        for itemIndex in range(self.n_items):\n",
    "\n",
    "            processedItems += 1\n",
    "\n",
    "            if processedItems % 10000==0 or processedItems==self.n_items:\n",
    "\n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "                print(\"Similarity item {} ( {:2.0f} % ), {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                    processedItems, processedItems*1.0/self.n_items*100, itemPerSec, (self.n_items-processedItems) / itemPerSec / 60))\n",
    "\n",
    "            this_item_weights = self.computeItemSimilarities(itemIndex)\n",
    "\n",
    "            if self.TopK == 0:\n",
    "\n",
    "                for innerItemIndex in range(self.n_items):\n",
    "                    self.W_dense[innerItemIndex,itemIndex] = this_item_weights[innerItemIndex]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Sort indices and select TopK\n",
    "                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\n",
    "                # top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\n",
    "\n",
    "                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                # because we avoid sorting elements we already know we don't care about\n",
    "                # - Partition the data to extract the set of TopK items, this set is unsorted\n",
    "                # - Sort only the TopK items, discarding the rest\n",
    "                # - Get the original item index\n",
    "\n",
    "                this_item_weights_np = - np.array(this_item_weights)\n",
    "                \n",
    "                # Get the unordered set of topK items\n",
    "                top_k_partition = np.argpartition(this_item_weights_np, self.TopK-1)[0:self.TopK]\n",
    "                # Sort only the elements in the partition\n",
    "                top_k_partition_sorting = np.argsort(this_item_weights_np[top_k_partition])\n",
    "                # Get original index\n",
    "                top_k_idx = top_k_partition[top_k_partition_sorting]\n",
    "\n",
    "\n",
    "\n",
    "                # Incrementally build sparse matrix\n",
    "                for innerItemIndex in range(len(top_k_idx)):\n",
    "\n",
    "                    topKItemIndex = top_k_idx[innerItemIndex]\n",
    "\n",
    "                    values[sparse_data_pointer] = this_item_weights[topKItemIndex]\n",
    "                    rows[sparse_data_pointer] = topKItemIndex\n",
    "                    cols[sparse_data_pointer] = itemIndex\n",
    "\n",
    "                    sparse_data_pointer += 1\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "\n",
    "            return np.array(self.W_dense)\n",
    "\n",
    "        else:\n",
    "\n",
    "            values = np.array(values[0:sparse_data_pointer])\n",
    "            rows = np.array(rows[0:sparse_data_pointer])\n",
    "            cols = np.array(cols[0:sparse_data_pointer])\n",
    "\n",
    "            W_sparse = sps.csr_matrix((values, (rows, cols)),\n",
    "                                    shape=(self.n_items, self.n_items),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 10000 ( 15 % ), 722.73 item/sec, required time 1.27 min\n",
      "Similarity item 20000 ( 31 % ), 1152.12 item/sec, required time 0.65 min\n",
      "Similarity item 30000 ( 46 % ), 1413.59 item/sec, required time 0.41 min\n",
      "Similarity item 40000 ( 61 % ), 1611.02 item/sec, required time 0.26 min\n",
      "Similarity item 50000 ( 77 % ), 1761.78 item/sec, required time 0.14 min\n",
      "Similarity item 60000 ( 92 % ), 1876.49 item/sec, required time 0.05 min\n",
      "Similarity item 65134 ( 100 % ), 1929.34 item/sec, required time 0.00 min\n",
      "Similarity computed in 33.94 seconds\n"
     ]
    }
   ],
   "source": [
    "cosine_cython = Cosine_Similarity(URM_train, TopK=100)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cosine_cython.compute_similarity()\n",
    "\n",
    "print(\"Similarity computed in {:.2f} seconds\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Better... much better. There are a few other things you could do, but at this point it is not worth the effort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How to use Cython outside a notebook\n",
    "\n",
    "### Step1: Create a .pyx file and write your code\n",
    "\n",
    "### Step2: Create a compilation script \"compileCython.py\" with the following content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will not run in a notebook cell\n",
    "\n",
    "try:\n",
    "    from setuptools import setup\n",
    "    from setuptools import Extension\n",
    "except ImportError:\n",
    "    from distutils.core import setup\n",
    "    from distutils.extension import Extension\n",
    "\n",
    "\n",
    "from Cython.Distutils import build_ext\n",
    "import numpy\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "if len(sys.argv) != 4:\n",
    "    raise ValueError(\"Wrong number of paramethers received. Expected 4, got {}\".format(sys.argv))\n",
    "\n",
    "\n",
    "# Get the name of the file to compile\n",
    "fileToCompile = sys.argv[1]\n",
    "\n",
    "# Remove the argument from sys argv in order for it to contain only what setup needs\n",
    "del sys.argv[1]\n",
    "\n",
    "extensionName = re.sub(\"\\.pyx\", \"\", fileToCompile)\n",
    "\n",
    "\n",
    "ext_modules = Extension(extensionName,\n",
    "                [fileToCompile],\n",
    "                extra_compile_args=['-O3'],\n",
    "                include_dirs=[numpy.get_include(),],\n",
    "                )\n",
    "\n",
    "setup(\n",
    "    cmdclass={'build_ext': build_ext},\n",
    "    ext_modules=[ext_modules]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Compile your code with the following command \n",
    "\n",
    "python compileCython.py Cosine_Similarity_Cython.pyx build_ext --inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Generate cython report and look for \"yellow lines\". The report is an .html file which represents how many operations are necessary to translate each python operation in cython code. If a line is white, it has a direct C translation. If it is yellow it will require many indirect steps that will slow down execution. Some of those steps may be inevitable, some may be removed via static typing.\n",
    "\n",
    "### IMPORTANT: white does not mean fast!! If a system call is involved that part might be slow anyway.\n",
    "\n",
    "cython -a Cosine_Similarity_Cython.pyx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: Add static types and C functions to remove \"yellow\" lines.\n",
    "\n",
    "#### If you use a variable only as a C object, use primitive tipes \n",
    "cdef int namevar\n",
    "\n",
    "def double namevar\n",
    "\n",
    "cdef float namevar\n",
    "\n",
    "#### If you call a function only within C code, use a specific declaration \"cdef\"\n",
    "\n",
    "cdef function_name(self, int param1, double param2):\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6: Iterate step 4 and 5 until you are satisfied with how clean your code is, then compile. An example of non optimized code can be found in the source folder of this notebook with the _SLOW suffix\n",
    "\n",
    "## Step7: the compilation generates a file wose name is something like \"Cosine_Similarity_Cython.cpython-36m-x86_64-linux-gnu.so\" and tells you the source file, the architecture it is compiled for and the OS\n",
    "\n",
    "## Step8: Import and use the compiled file as if it were a python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Base.Simialrity.Cython.Cosine_Similarity_Cython import Cosine_Similarity\n",
    "\n",
    "cosine_cython = Cosine_Similarity(URM_train, TopK=100)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cosine_cython.compute_similarity()\n",
    "\n",
    "print(\"Similarity computed in {:.2f} seconds\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2018/19\n",
    "\n",
    "### Practice session on MF PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "# skip the download\n",
    "#urlretrieve (\"http://files.grouplens.org/datasets/movielens/ml-10m.zip\", \"data/Movielens_10M/movielens_10m.zip\")\n",
    "dataFile = zipfile.ZipFile(\"data/Movielens_10M/movielens_10m.zip\")\n",
    "URM_path = dataFile.extract(\"ml-10M100K/ratings.dat\", path = \"data/Movielens_10M\")\n",
    "URM_file = open(URM_path, 'r')\n",
    "\n",
    "\n",
    "def rowSplit (rowString):\n",
    "    \n",
    "    split = rowString.split(\"::\")\n",
    "    split[3] = split[3].replace(\"\\n\",\"\")\n",
    "    \n",
    "    split[0] = int(split[0])\n",
    "    split[1] = int(split[1])\n",
    "    split[2] = float(split[2])\n",
    "    split[3] = int(split[3])\n",
    "    \n",
    "    result = tuple(split)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "URM_file.seek(0)\n",
    "URM_tuples = []\n",
    "\n",
    "for line in URM_file:\n",
    "   URM_tuples.append(rowSplit (line))\n",
    "\n",
    "userList, itemList, ratingList, timestampList = zip(*URM_tuples)\n",
    "\n",
    "userList = list(userList)\n",
    "itemList = list(itemList)\n",
    "ratingList = list(ratingList)\n",
    "timestampList = list(timestampList)\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all = URM_all.tocsr()\n",
    "\n",
    "\n",
    "\n",
    "from Notebooks_utils.data_splitter import train_test_holdout\n",
    "\n",
    "\n",
    "URM_train, URM_test = train_test_holdout(URM_all, train_perc = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF models rely upon latent factors for users and items which are called 'embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = 10\n",
    "\n",
    "n_users, n_items = URM_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "user_factors = torch.nn.Embedding(num_embeddings = n_users, embedding_dim = num_factors)\n",
    "item_factors = torch.nn.Embedding(num_embeddings = n_items, embedding_dim = num_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(71568, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(65134, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute the prediction we have to multiply the user factors to the item factors, which is a linear operation.\n",
    "\n",
    "### We define a single layer and an activation function, which takes the result and transforms it in the final prediction. The activation function can be used to restrict the predicted values (e.g., sigmoid is between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=1, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1 = torch.nn.Linear(in_features = num_factors, out_features = 1)\n",
    "\n",
    "layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_function = torch.nn.ReLU()\n",
    "\n",
    "activation_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to compute the prediction you have to:\n",
    "* Define a list of user and item indices\n",
    "* Create a tensor from it\n",
    "* Create a variable from the tensor\n",
    "* Get the user and item embedding\n",
    "* Compute the element-wise product of the embeddings\n",
    "* Pass the element-wise product to the single layer network\n",
    "* Pass the output of the single layer network to the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0934,  0.3721, -0.0781, -1.2313,  0.8013, -0.5451, -0.7487,\n",
       "         -1.1881,  0.0999,  0.8536]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "item_index = [15]\n",
    "user_index = [42]\n",
    "\n",
    "user_index = torch.Tensor(user_index).type(torch.LongTensor)\n",
    "item_index = torch.Tensor(item_index).type(torch.LongTensor)\n",
    "\n",
    "user_index = Variable(user_index)\n",
    "item_index = Variable(item_index)\n",
    "\n",
    "current_user_factors = user_factors(user_index)\n",
    "current_item_factors = item_factors(item_index)\n",
    "\n",
    "element_wise_product = torch.mul(current_user_factors, current_item_factors)\n",
    "element_wise_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To take the result of the prediction and transform it into a traditional numpy array you have to first call .detach() and then .numpy()\n",
    "### The result is an array of 1 cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is [[0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction = layer_1(element_wise_product)\n",
    "prediction = activation_function(prediction)\n",
    "\n",
    "prediction_numpy = prediction.detach().numpy()\n",
    "\n",
    "print(\"Prediction is {}\".format(prediction_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a MF MSE model with PyTorch\n",
    "\n",
    "# Step 1 Create a Model python object\n",
    "\n",
    "### The model should implement the forward function which computes the prediction as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MF_MSE_PyTorch_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "\n",
    "        super(MF_MSE_PyTorch_model, self).__init__()\n",
    "\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "\n",
    "        self.user_factors = torch.nn.Embedding(num_embeddings = self.n_users, embedding_dim = self.n_factors)\n",
    "        self.item_factors = torch.nn.Embedding(num_embeddings = self.n_items, embedding_dim = self.n_factors)\n",
    "\n",
    "        self.layer_1 = torch.nn.Linear(in_features = self.n_factors, out_features = 1)\n",
    "\n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, user_coordinates, item_coordinates):\n",
    "\n",
    "        current_user_factors = self.user_factors(user_coordinates)\n",
    "        current_item_factors = self.item_factors(item_coordinates)\n",
    "\n",
    "        prediction = torch.mul(current_user_factors, current_item_factors)\n",
    "\n",
    "        prediction = self.layer_1(prediction)\n",
    "        prediction = self.activation_function(prediction)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "\n",
    "    def get_W(self):\n",
    "\n",
    "        return self.user_factors.weight.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    def get_H(self):\n",
    "\n",
    "        return self.item_factors.weight.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Setup PyTorch devices and Data Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF_MSE_PyTorch: Using CPU\n"
     ]
    }
   ],
   "source": [
    "use_cuda = False\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"MF_MSE_PyTorch: Using CUDA\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"MF_MSE_PyTorch: Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the model and specify the device it should run on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyTorchModel = MF_MSE_PyTorch_model(n_users, n_items, num_factors).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose loss functions, there are quite a few to choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunction = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the optimizer to be used for the model parameters: Adam, AdaGrad, RMSProp etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adagrad(pyTorchModel.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DatasetIterator, which will be used to iterate over the data\n",
    "\n",
    "### A DatasetIterator will implement the Dataset class and provide the __getitem__(self, index) method, which allows to get the data points indexed by that index.\n",
    "\n",
    "### Since we need the data to be a tensor, we pre inizialize everything as a tensor. In practice we save the URM in coordinate format (user, item, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class DatasetIterator_URM(Dataset):\n",
    "\n",
    "    def __init__(self, URM):\n",
    "\n",
    "        URM = URM.tocoo()\n",
    "\n",
    "        self.n_data_points = URM.nnz\n",
    "\n",
    "        self.user_item_coordinates = np.empty((self.n_data_points, 2))\n",
    "\n",
    "        self.user_item_coordinates[:,0] = URM.row.copy()\n",
    "        self.user_item_coordinates[:,1] = URM.col.copy()\n",
    "        self.rating = URM.data.copy().astype(np.float)\n",
    "\n",
    "        self.user_item_coordinates = torch.Tensor(self.user_item_coordinates).type(torch.LongTensor)\n",
    "        self.rating = torch.Tensor(self.rating)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Format is (row, col, data)\n",
    "        :param index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        return self.user_item_coordinates[index, :], self.rating[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.n_data_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We pass the DatasetIterator to a DataLoader object which manages the use of batches and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "dataset_iterator = DatasetIterator_URM(URM_train)\n",
    "\n",
    "train_data_loader = DataLoader(dataset = dataset_iterator,\n",
    "                   batch_size = batch_size,\n",
    "                   shuffle = True,\n",
    "                   #num_workers = 2,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now we ran the usual epoch steps\n",
    "* Data point sampling\n",
    "* Prediction computation\n",
    "* Loss function computation\n",
    "* Gradient computation\n",
    "* Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 24994, loss 1299.5726\n",
      "Batch 100 of 24994, loss 1246.1948\n",
      "Batch 200 of 24994, loss 1325.1472\n",
      "Batch 300 of 24994, loss 1316.5590\n",
      "Batch 400 of 24994, loss 1184.6915\n",
      "Batch 500 of 24994, loss 1412.0217\n",
      "Batch 600 of 24994, loss 1351.8953\n",
      "Batch 700 of 24994, loss 1268.8069\n",
      "Batch 800 of 24994, loss 1375.4374\n",
      "Batch 900 of 24994, loss 1439.9543\n",
      "Batch 1000 of 24994, loss 1506.6323\n",
      "Batch 1100 of 24994, loss 1488.0021\n",
      "Batch 1200 of 24994, loss 1183.2865\n",
      "Batch 1300 of 24994, loss 1492.3884\n",
      "Batch 1400 of 24994, loss 1337.6088\n",
      "Batch 1500 of 24994, loss 1466.0822\n",
      "Batch 1600 of 24994, loss 1326.6989\n",
      "Batch 1700 of 24994, loss 1308.6276\n",
      "Batch 1800 of 24994, loss 1500.3684\n",
      "Batch 1900 of 24994, loss 1470.4662\n",
      "Batch 2000 of 24994, loss 1407.8556\n",
      "Interrupting train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for num_batch, (input_data, label) in enumerate(train_data_loader, 0):\n",
    "    \n",
    "    cumulative_loss = 0\n",
    "\n",
    "    # On windows requires int64, on ubuntu int32\n",
    "    #input_data_tensor = Variable(torch.from_numpy(np.asarray(input_data, dtype=np.int64))).to(self.device)\n",
    "    input_data_tensor = Variable(input_data).to(device)\n",
    "\n",
    "    label_tensor = Variable(label).to(device)\n",
    "\n",
    "\n",
    "    user_coordinates = input_data_tensor[:,0]\n",
    "    item_coordinates = input_data_tensor[:,1]\n",
    "\n",
    "    # FORWARD pass\n",
    "    prediction = pyTorchModel(user_coordinates, item_coordinates)\n",
    "\n",
    "    # Pass prediction and label removing last empty dimension of prediction\n",
    "    loss = lossFunction(prediction.view(-1), label_tensor)\n",
    "    \n",
    "\n",
    "    if num_batch % 100 == 0:\n",
    "        \n",
    "        print(\"Batch {} of {}, loss {:.4f}\".format(num_batch, len(train_data_loader), loss.data.item()))\n",
    "        \n",
    "        if num_batch == 2000:\n",
    "            print(\"Interrupting train\")\n",
    "            break\n",
    "    \n",
    "\n",
    "    # BACKWARD pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the train is complete (it may take a while and many epochs), we can get the matrices in the usual numpy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = pyTorchModel.get_W()\n",
    "H = pyTorchModel.get_H()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8693999 ,  0.05498629, -0.07996137, ...,  0.80227995,\n",
       "         0.16469592,  1.5831672 ],\n",
       "       [-1.7885368 , -1.3893236 , -0.19196971, ..., -1.0810672 ,\n",
       "         0.70827496,  0.24173023],\n",
       "       [-0.36013108, -1.4618611 , -0.53237206, ...,  0.7356418 ,\n",
       "        -0.42919588, -0.02103774],\n",
       "       ...,\n",
       "       [ 0.17021644, -0.45737997, -1.2599324 , ...,  1.7705928 ,\n",
       "         0.47563678, -0.2838048 ],\n",
       "       [ 0.00311422, -0.40563554, -2.5473928 , ..., -1.3527074 ,\n",
       "         1.0070924 , -0.5943767 ],\n",
       "       [ 0.18628068, -0.26068607, -0.49782896, ..., -1.469555  ,\n",
       "         0.2341343 ,  0.53745097]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5917609 ,  0.6629568 ,  0.8474942 , ..., -1.2384548 ,\n",
       "         0.3644825 , -0.8415124 ],\n",
       "       [ 2.1821175 , -2.0746005 ,  0.59765303, ..., -0.32873613,\n",
       "        -0.94762105, -0.57247436],\n",
       "       [-0.7156964 ,  1.0119928 ,  0.7251737 , ..., -0.19364427,\n",
       "         1.2719904 ,  1.1096959 ],\n",
       "       ...,\n",
       "       [ 0.46778318, -0.18097553, -0.9623822 , ..., -0.26983652,\n",
       "         0.12652689,  2.4998083 ],\n",
       "       [ 0.4835092 ,  1.2725722 , -1.172361  , ...,  0.23162012,\n",
       "        -1.3099946 , -2.0804546 ],\n",
       "       [ 0.4059417 , -0.0283024 ,  1.3251573 , ..., -0.17705719,\n",
       "        -0.75827426, -0.07519675]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
